\relax 
\babel@aux{english}{}
\citation{lucene-wiki}
\citation{lucene}
\@writefile{toc}{\contentsline {section}{\numberline {1}Lucene}{2}\protected@file@percent }
\newlabel{sec:lucene}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Dataset}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Analyzer and Setup}{2}\protected@file@percent }
\newlabel{sec:analysetup}{{1.2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Indexing}{3}\protected@file@percent }
\newlabel{sec:indexing}{{1.3}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Index Directory}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Indexing of Files}{3}\protected@file@percent }
\newlabel{sec:indexfiles}{{1.3.2}{3}}
\citation{lucene-tutorial}
\citation{lucene}
\citation{lucene}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}Storing the Feature Vectors}{4}\protected@file@percent }
\newlabel{sec:fv}{{1.3.3}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.4}Index File Formats}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Score Models}{4}\protected@file@percent }
\citation{lucene}
<<<<<<< Updated upstream
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Similarity Scoring}{5}\protected@file@percent }
\newlabel{sec:ss}{{1.4.1}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Query Building}{5}\protected@file@percent }
\newlabel{sec:query}{{2}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}String Replacements}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}String Separation}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Spell Correction}{7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces WolframAlpha-made plot of both $y=36^{x}$ and $y=2^{2x-1}$.}}{8}\protected@file@percent }
=======
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Similarity Scoring}{5}}
\newlabel{sec:sim}{{1.4.1}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Query Building}{5}}
\newlabel{sec:query}{{2}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}String Replacements}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}String Separation}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Spell Correction}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces WolframAlpha-made plot of both $y=36^{x}$ and $y=2^{2x-1}$.}}{8}}
>>>>>>> Stashed changes
\newlabel{fig:wa}{{1}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Benchmark Study}{10}\protected@file@percent }
\newlabel{sec:bstudy}{{3}{10}}
<<<<<<< Updated upstream
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Manual Labeling}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Tags as Queries}{10}\protected@file@percent }
\citation{hapax}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Performance Scoring}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}PR and ROC}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Initial Score}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Scoring Models}{12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Results initial Benchmark Performance}}{13}\protected@file@percent }
\newlabel{fig:initBP}{{2}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Optimizations}{13}\protected@file@percent }
\newlabel{sec:optimizations}{{4}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Removing Special Characters and Whitespace}{13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces An overview of the standard Analyzer using a Boolean Similarity measurement}}{14}\protected@file@percent }
\newlabel{fig:bool}{{3}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces An overview of the standard Analyzer using a Tf-Idf Similarity measurement}}{15}\protected@file@percent }
\newlabel{fig:tfidf}{{4}{15}}
\citation{lucene}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Results Benchmark Performance with Initial Filters}}{16}\protected@file@percent }
\newlabel{fig:alphanumeric}{{5}{16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Word Delimiter}{16}\protected@file@percent }
\newlabel{sec:word-delimiter}{{4.1.1}{16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Stemming}{17}\protected@file@percent }
\newlabel{sec:stemming}{{4.1.2}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Results Analyzer extended with Stemming}}{17}\protected@file@percent }
\newlabel{fig:stemming}{{6}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}$n$-grams}{17}\protected@file@percent }
\newlabel{sec:ngrams}{{4.1.3}{17}}
\citation{synonyms}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces An overview of the extended Analyzer for $n$-gram filtering, where $n = 2$. $AUC = 0.5$}}{18}\protected@file@percent }
\newlabel{fig:ngram2}{{7}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Manual Thesaurus-Based Query Expansion}{18}\protected@file@percent }
\newlabel{sec:synonyms}{{4.2}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Final Results}{18}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces An overview of the extended Analyzer for $n$-gram filtering, where $n = 3$. $AUC = 0.5$}}{19}\protected@file@percent }
\newlabel{fig:ngram3}{{8}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces An overview of the extended Analyzer for $n$-gram filtering, where $n = 5$. $AUC = 0.46$}}{19}\protected@file@percent }
\newlabel{fig:ngram5}{{9}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces An overview of the extended Analyzer for $n$-gram filtering, where $n = 6$. $AUC = 0.46$}}{20}\protected@file@percent }
\newlabel{fig:ngram6}{{10}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces An overview of the extended Analyzer for $n$-gram filtering, where $n = 8$. $AUC = 0.60$}}{20}\protected@file@percent }
\newlabel{fig:ngram8}{{11}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces An overview of the extended Analyzer for manual thesaurus-based query expansion. $AUC = 0.52$}}{21}\protected@file@percent }
\newlabel{fig:synonyms}{{12}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Results Benchmarking with word delimiters and stemming.}}{21}\protected@file@percent }
\newlabel{fig:final}{{13}{21}}
\citation{lucene}
\@writefile{toc}{\contentsline {section}{\numberline {5}Rocchio Algorithm}{22}\protected@file@percent }
\newlabel{sec:rocchio}{{5}{22}}
=======
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Manual Labeling}{10}}
\citation{hapax}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Tags as Queries}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Performance Scoring}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}PR and ROC}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Initial Score}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Scoring Models}{12}}
\newlabel{sec:scmodels}{{3.3}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Results initial Benchmark Performance. $AUC = 0.74$}}{13}}
\newlabel{fig:initBP}{{2}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Optimizations}{13}}
\newlabel{sec:optimizations}{{4}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Removing Special Characters and Whitespace}{13}}
\citation{lucene}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Results Benchmark Performance with Initial Filters. $AUC = 0.68$}}{14}}
\newlabel{fig:alphanumeric}{{3}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Word Delimiter}{14}}
\newlabel{sec:word-delimiter}{{4.1.1}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Stemming}{14}}
\newlabel{sec:stemming}{{4.1.2}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Results Analyzer extended with Stemming}}{15}}
\newlabel{fig:stemming}{{4}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}$n$-grams}{15}}
\newlabel{sec:ngrams}{{4.1.3}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces An overview of the extended Analyzer for $n$-gram filtering, where $n = 2$. $AUC = 0.5$}}{16}}
\newlabel{fig:ngram2}{{5}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces An overview of the extended Analyzer for $n$-gram filtering, where $n = 3$. $AUC = 0.5$}}{16}}
\newlabel{fig:ngram3}{{6}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces An overview of the extended Analyzer for $n$-gram filtering, where $n = 5$. $AUC = 0.46$}}{17}}
\newlabel{fig:ngram5}{{7}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces An overview of the extended Analyzer for $n$-gram filtering, where $n = 6$. $AUC = 0.46$}}{17}}
\newlabel{fig:ngram6}{{8}{17}}
\citation{synonyms}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces An overview of the extended Analyzer for $n$-gram filtering, where $n = 8$. $AUC = 0.60$}}{18}}
\newlabel{fig:ngram8}{{9}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Manual Thesaurus-Based Query Expansion}{18}}
\newlabel{sec:synonyms}{{4.2}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Final Results}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces An overview of the extended Analyzer for manual thesaurus-based query expansion. $AUC = 0.52$}}{19}}
\newlabel{fig:synonyms}{{10}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Results Benchmarking with word delimiters and stemming.}}{19}}
\newlabel{fig:final}{{11}{19}}
\citation{lucene}
\@writefile{toc}{\contentsline {section}{\numberline {5}Rocchio Algorithm}{20}}
\newlabel{sec:rocchio}{{5}{20}}
>>>>>>> Stashed changes
\bibstyle{abbrv}
\bibdata{mybib}
\bibcite{lucene}{1}
\bibcite{lucene-wiki}{2}
\bibcite{hapax}{3}
\bibcite{lucene-tutorial}{4}
\bibcite{synonyms}{5}
