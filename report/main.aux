\relax 
\babel@aux{english}{}
\citation{lucene-wiki}
\citation{lucene}
\@writefile{toc}{\contentsline {section}{\numberline {1}Lucene}{2}}
\newlabel{sec:lucene}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Dataset}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Analyzer and Setup}{2}}
\newlabel{sec:analysetup}{{1.2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Indexing}{3}}
\newlabel{sec:indexing}{{1.3}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Index Directory}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Indexing of Files}{3}}
\newlabel{sec:indexfiles}{{1.3.2}{3}}
\citation{lucene-tutorial}
\citation{lucene}
\citation{lucene}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}Storing the Feature Vectors}{4}}
\newlabel{sec:fv}{{1.3.3}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.4}Index File Formats}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Score Models}{4}}
\citation{lucene}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Similarity Scoring}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Query Building}{5}}
\newlabel{sec:query}{{2}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}String Replacements}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}String Separation}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Spell Correction}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces WolframAlpha-made plot of both $y=36^{x}$ and $y=2^{2x-1}$.}}{8}}
\newlabel{fig:wa}{{1}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Benchmark Study}{10}}
\newlabel{sec:bstudy}{{3}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Manual Labeling}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Tags as Queries}{10}}
\citation{hapax}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Performance Scoring}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}PR and ROC}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Initial Score}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Scoring Models}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Optimizations}{12}}
\newlabel{sec:optimizations}{{4}{12}}
\citation{lucene}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Results initial Benchmark Performance}}{13}}
\newlabel{fig:initBP}{{2}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Removing Special Characters and Whitespace}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Results Benchmark Performance with Initial Filters}}{14}}
\newlabel{fig:alphanumeric}{{3}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Word Delimiter}{14}}
\newlabel{sec:word-delimiter}{{4.1.1}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Stemming}{14}}
\newlabel{sec:stemming}{{4.1.2}{14}}
\citation{synonyms}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Results Analyzer extended with Stemming}}{15}}
\newlabel{fig:stemming}{{4}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}$n$-grams}{15}}
\newlabel{sec:ngrams}{{4.1.3}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Manual Thesaurus-Based Query Expansion}{15}}
\newlabel{sec:synonyms}{{4.2}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces An overview of the extended Analyzer for $n$-gram filtering, where $n = 2$. $AUC = 0.5$}}{16}}
\newlabel{fig:ngram2}{{5}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces An overview of the extended Analyzer for $n$-gram filtering, where $n = 3$. $AUC = 0.5$}}{16}}
\newlabel{fig:ngram3}{{6}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces An overview of the extended Analyzer for $n$-gram filtering, where $n = 5$. $AUC = 0.46$}}{17}}
\newlabel{fig:ngram5}{{7}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces An overview of the extended Analyzer for $n$-gram filtering, where $n = 6$. $AUC = 0.46$}}{17}}
\newlabel{fig:ngram6}{{8}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces An overview of the extended Analyzer for $n$-gram filtering, where $n = 8$. $AUC = 0.60$}}{18}}
\newlabel{fig:ngram8}{{9}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Final Results}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Rocchio Algorithm}{18}}
\newlabel{sec:rocchio}{{5}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces An overview of the extended Analyzer for manual thesaurus-based query expansion. $AUC = 0.52$}}{19}}
\newlabel{fig:synonyms}{{10}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Results Benchmarking with word delimiters and stemming.}}{19}}
\newlabel{fig:final}{{11}{19}}
\citation{lucene}
\bibstyle{abbrv}
\bibdata{mybib}
\bibcite{lucene}{1}
\bibcite{lucene-wiki}{2}
\bibcite{hapax}{3}
\bibcite{lucene-tutorial}{4}
\bibcite{synonyms}{5}
