\documentclass[11pt]{article}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{csquotes}
\usepackage{amsmath}
\usepackage{url}
\usepackage[english]{babel} % Valid line breaking
\usepackage{subfig}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{JavaStyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=JavaStyle}

\renewcommand{\today}{\ifcase \month \or January\or February\or March\or April\or May\or June\or July\or August\or September\or October\or November\or December\fi, \number \year} 
\newcommand{\topic}{Project Lucene}
\newcommand{\paper}{StackOverflow\\
	\small{\textit{M INF \  2019-2020}}
}
\newcommand{\team}{
	\=Johannes Akkermans%, \\\>Randy Paredis
}

\begin{document}
\title{\textmd{\textbf{Information Retrieval}}\\\normalsize\vspace{0.1in}\Large{\topic}\\\large{\paper}\Large\\\vspace{0.1in}}
\author{}
\date{\today}
\maketitle
\begin{center}
	\parbox{0cm}{
		\begin{tabbing}
		\textbf{Group:} \team
		\end{tabbing}
	}
\end{center}

\vspace*{1em}

\begin{abstract}
    Lucene is a powerful Java-tool that allows for easy information retrieval of large datasets. The goal of this project was to apply Lucene on the StackOverflow data and study its feasability. Additionally, we will try to improve the overall performance of the retrieval by applying some techniques we've seen in class.
\end{abstract}

\newpage

% Study the feasibility of using Lucene to add retrieval capabilities to the StackOverflow repository provided to you. Describe the functionality offered by Lucene, such as the types of indices that are included, the different score models, etc. How does Lucene store the index, does it have spell correction? Include a benchmarking study of the performance of the retrieval performance. Manually label some documents. One acceptable approach is to use the titles of the questions as the query, and only index the remaining parts of the documents. In this way a ground truth can be generated rather easily. This first part is expected to be about 10 pages.

\section{Lucene}\label{sec:lucene}
Apache Lucene is a free and open-source search engine software library written in Java. It allows for full-text indexing and com.stackoverflow.searching making it a perfect fit for the project at hand. \cite{lucene-wiki}

There are a lot of features Lucene offers, but we will not need all of them. This is why we will gradually go over our project and the Lucene features we've used to obtain the described results. All specific details on the internal workings of Lucene are described in its documentation, publicly available on \cite{lucene}.

\subsection{Dataset}
Of course, before we can do anything, we require a dataset on which we will execute Lucene. In order to do that, we've downloaded the massive dataset from StackOverflow\footnote{From the website indicated in the assignment.}. Next, we extracted this dataset before running a Python script that parses this massive xml-file into $2\,010\,607$ smaller files, containing questions and answers, all of whom had \texttt{python} or \texttt{c++} in its tags. Every 1000 files are put into a sub-folder, making sure our operating systems can handle this amount of data. The overall size approximates 7 gigabytes, which is why it's not accompanied with our submission of the project.

Now that we have this dataset, we can start looking at Lucene.

\subsection{Analyzer and Setup}\label{sec:analysetup}
The first and foremost thing to do in Lucene is to create an \texttt{Analyzer}. As the name might suggest, it will analyze text by building \texttt{TokenStreams}\footnote{\texttt{org.apache.lucene.analysis.TokenStream}}. More theoretically, it describes a policy on how items are extracted from text.

Lucene comes with a \texttt{StandardAnalyzer} class\footnote{\texttt{org.apache.lucene.analysis.standard.StandardAnalyzer}}. This subclass of the abstract \texttt{Analyzer} has a very broad use case, seeing as it correctly splits unicode text, turns everything to lowercase and removes a set of English stop words. It can be created quite easily:
\begin{lstlisting}[language=Java]
StandardAnalyzer analyzer = new StandardAnalyzer();
\end{lstlisting}

Alas, the \texttt{StandardAnalyzer} does not take into account that our data consists of a lot of xml-files. On top of that, xml makes use of a special character encoding system that does not map straightforwardly on unicode. This is an issue we need to keep in mind when processing our data. Luckily, there is the possibility of creating a custom \texttt{Analyzer} (in our project, this is the \texttt{MyCustomAnalyzer} class) to solve this issue. More on the \texttt{MyCustomAnalyzer} in section \ref{sec:optimizations}.

\subsection{Indexing}\label{sec:indexing}
\subsubsection{Index Directory}
Now that we have an \texttt{Analyzer}, we need a place to keep track of the indexing. Because indexing itself is an expensive operation, it will not be done before every search and, on top of that, it will be stored in a folder on disk, making sure your indexes remain consistent (and above all: existent) between multiple executions.

In our implementation, we create a new directory, called \texttt{.search} in which we will store this information. If this directory already exists, it is cleared beforehand. Note that, this way, we will bypass the usecase of storing these files in a directory.
\begin{lstlisting}[language=Java]
Directory index = new MMapDirectory(Paths.get(".search"));
\end{lstlisting}

\subsubsection{Indexing of Files}\label{sec:indexfiles}
The dataset we've used was too big to index as a whole for the scope of our project. This is why our code only indexes on a part of this data ($10\,000$ files), which was randomly selected using Java's builtin random number generator with a fixed seed of 420 (for repeatability).

In order to do so, we had to flatten our dataset in such a way that our OS could handle it. Here is where we discovered a problem on MacOS (compared to Ubuntu LTS 18.04, that is). On Mac, the \texttt{File} class actually opens the file, causing this to be a costly operation. We solved this inconvenience via iterating over the filenames and only taking those ending in ``xml''\footnote{This method has as a downside that it will not work on Windows systems. Mainly because we hardcoded the ``\texttt{/}'' as a path separator.}.

Either way, after we've obtained our files to index, we will add all these documents to an \texttt{IndexWriter}\footnote{\texttt{org.apache.lucene.index.IndexWriter}}. This object basically keeps track of all labeled information of the documents in our dataset.
\begin{lstlisting}[language=Java]
IndexWriterConfig config = new IndexWriterConfig(analyzer);
IndexWriter w = new IndexWriter(index, config);
\end{lstlisting}

For the purpose of the assignment, we indexed the fields \textit{name} (i.e. the filename), \textit{title}, \textit{body}, \textit{tags} and \textit{answers}. These last four need to be transformed in such a way that all special characters and capitalization are removed (as mentioned in section \ref{sec:analysetup}, see also section \ref{sec:query}). This is done by our \texttt{MyCustomAnalyzer} (as will be discussed in section \ref{sec:optimizations}). Additionally, the \textit{answers} are all joined together, separated with two newline characters (\texttt{\textbackslash n}), for ease of use.

Internally, Lucene does not search all the files, because that would be too costly. In fact, this indexing stage creates a list of indexes and the pages they refer to, similarly to something you might see in a handbook. These indexes are called ``\textit{reverse indexes}''. \cite{lucene-tutorial}

\subsubsection{Storing the Feature Vectors}\label{sec:fv}
Because we will need it later on (see section \ref{sec:rocchio}), we will also tell Lucene it needs to store the term vectors of all fields. This way, we hope to be able to use the Vector Space Model (VSM) Lucene uses behind the scenes.

\subsubsection{Index File Formats}
As mentioned before, Lucene stores the indexing data in a set of files. Its contents are not entirely readable by a human\footnote{Using ourselves as a point of reference.}, making us suspect there is some sort compression or encoding that's being done by Lucene. Our suspicions were confirmed in \cite{lucene}, which also gives an extended overview of this encoding.

\subsection{Score Models}
There are numerous scoring models bundled with Lucene (as we will discuss in section \ref{sec:scmodels}). By default, Lucene uses the \texttt{TFIDFSimilarity}\footnote{\texttt{org.apache.lucene.search.similarities.TFIDFSimilarity}}.

The fastest way to obtain a score in Lucene, would by via the following code:
\begin{lstlisting}[language=Java]
// Search with the query
IndexReader reader = DirectoryReader.open(index);
IndexSearcher searcher = new IndexSearcher(reader);
TopDocs docs = searcher.search(q, cnt);
ScoreDoc[] hits = docs.scoreDocs;
\end{lstlisting}

Here, we would search for a query \texttt{q} and return at most \texttt{cnt} documents that match it, ordered by highest score first. To quote scoring in the words from \cite{lucene}, which also describes our love-hate relationship with it:
\begin{displayquote}
    \textsl{Lucene scoring is the heart of why we all love Lucene. It is blazingly fast and it hides almost all of the complexity from the user. In a nutshell, it works. At least, that is, until it doesn't work, or doesn't work as one would expect it to work.}
\end{displayquote}

Allow us to emphasize that this quote comes unedited from their own website, basically stating that Lucene does not always works the way you want it to work, an issue we have experienced on numerous occasions during this project (especially in section \ref{sec:rocchio}).

\subsubsection{Similarity Scoring}\label{sec:ss}
From \cite{lucene}, we can also conclude how its scoring actually works internally. From the course, we know there must be some similarity checking formula at work. We find that Lucene makes use of the \textsf{VSM score}, better known as the \textsf{cosine simularity} (with $V(q)$ and $V(d)$ weighed query vectors):
$$cosine\_similarity(q, d) = \frac{V(q)\cdot V(d)}{\vert V(q)\vert\cdot\vert V(d)\vert}$$
Its scoring formula therefore becomes:
\begin{align*}
    score(q, d) =&\ coord(q, d)\cdot queryNorm(q)\ \cdot\\
    &\sum_{t \in q}\left[tf(t\in d)\cdot idf(t)^2 \cdot t.getBoost()\cdot norm(t, d)\right]
\end{align*}

\section{Query Building}\label{sec:query}
Even though Lucene makes use of a Vector Space Model (VSM), it is by default completely hidden. On top of that, we haven't found any information (nor in the documentation, nor anywhere online) on how to search the dataset by vector. We would have to create our own scoring system, an idea we quickly discarded because of the massive size of our data\footnote{We also discarded this idea because we did not believe this was intended with the assignment. In fact, the fast and efficient workings of the similarity checking as described in section \ref{sec:ss} was a major influence in this decision.}. Instead, we decided to use the builtin binary query system, which is tested and optimized for com.stackoverflow.searching\footnote{On top of that, it is impossible to obtain the VSM without giving such a query string.}.

To allow for simple manipulations of the queries we introduced the notion of a \texttt{QueryBuilder}. Based on a string with the valid search criteria, we create a query that follows the Lucene query syntax and remains conform with the inputted data. Even though this may seem as an unnecessary additional parsing step, it will become useful later on.

Let's take a deeper dive into our \texttt{QueryBuilder}.

\subsection{String Replacements}
Before we even start to build our query, we need to transform a string into a general syntax that can be easily understood without too much issues. We identify three phases:

First we have the \textsf{trimming phase}. Here, we remove all whitespace surrounding our query, i.e. at the beginning and the end. Although the Lucene documentation does not explicitly state this as a requirement, we wanted to make sure it does not cause any undefined behaviour.

Next, we transform our string into lowercase format. Let's call this the \textsf{lowercase transformation phase}. This was done because matching on capitalization is not useful within our problem domain. Of course, this also requires our documents to be transformed in a similar manner (see sections \ref{sec:indexing} and \ref{sec:optimizations}).

Finally, there is the \textsf{alphanumerisation phase}\footnote{Or that's at least how we called it.}. Here, we will remove all special characters from our queries, seeing as they will only cause issues and strange behaviour (i.e. leaving a period in the query could result in the match of any character). Admittedly, in doing so we loose the ability to match hyphenated words and other special cases like the query ``\texttt{c++}''. It was our hypothesis that these words would not take up too much of the language and thus are irrelevant in this context.

Assuming our query is stored in the variable \texttt{word}, these three phases look, quite elegantly, like this:
\begin{lstlisting}[language=Java]
word.strip().toLowerCase().replaceAll("[^a-z0-9 ]", "")
\end{lstlisting}

Note that this only applies for the query itself. As will be discussed in section \ref{sec:optimizations}, for the documents themselves, this will be achieved via applying some additional filters to the analyzer.

\subsection{String Separation}
As you might have guessed from the previous section, we have only removed all whitespace from the beginning and the end of our string. All spaces in the middle are left and for good reason. They tell our system that all of those words \textbf{must} appear together.

If we look at Google and how they go about spaces, you might be able to deduce from the url, that they replace all spaces with an \texttt{AND}-gate (the plus sign). This not only allows words to be in a different word order, but also allows words not to be next to one another. This method works wonders for Google, so why wouldn't it be in our system?

Let's say we have the query ``\texttt{python database setup}''. By default, Lucene adds an \texttt{OR}-gate between these terms, but we want to enforce that these three words occur together (not necessarily in that order, or next to one another). Thus, we will add explicit \texttt{AND}-gates between them. Therefore, we obtain the query ``\texttt{python AND database AND setup}'' as an input to Lucene.

For the further query building, we will be looking at this list of whitespace-less strings, separated with an \texttt{AND}. Such an individual string will be referred to as a ``\textit{term}''.
\begin{lstlisting}[language=Java]
String[] terms = word.split(" ");
String newWord = String.join(" AND ", terms);
\end{lstlisting}

\subsection{Spell Correction}
Fuzzy search is a powerful tool. It allows a search engine to find words that are similar, just in case the user made a typo in the query. Fuzzy search can be referred to as ``\textit{Did You Mean...?}'' functionality.

In Lucene no spell checking is used by default, but there are multiple ways to obtain fuzzy search.
\begin{itemize}
    \item Use the Lucene query syntax for fuzzy search. This is done via adding the tilde ($\sim$) to the back of a term. In exploring how Lucene works and what influence this syntax had, we have found it does not fully accomplish what we want to achieve.\\
    By default, this syntax will allow words to be at most 50\% different. Within this context, the word ``\texttt{bear}'' will also match ``\texttt{beer}'', ``\texttt{boar}'', ``\texttt{bend}'', ``\texttt{fear}'', ``\texttt{feat}'', ``\texttt{rear}'' and ``\texttt{zeal}'' to name a few. All of the words mentioned above are real words, but the similarity measure does not make that assumption. Words like ``\texttt{glar}'', ``\texttt{qeaq}'' and ``\texttt{eeaa}'' does not exist in the English language, but will also be matched. Hypothetically, a set of documents on a fictional alien language might use these non-existing words a lot. If there are very few documents describing bears in the dataset, I might get a lot of fictional alien language information, while I did not want this.
    \item Use Lucene wildcards. While at first sight, this might be a good idea, on second thought there are way too many issues with this method. First, a term is not allowed to start with a wildcard (presumably because it would make it left-recursive). Secondly, there are way too many possible locations to add wildcards. Do you add them between all letters? Do you replace all letters? Do you do a combination of the two? For this combination method and an $n$-letter word, there are $2^{2(n - 1)}$ possibilities for adding wildcards. This is an upperbound, because there could be some overlap for some wildcards: \texttt{a**b} is equal to \texttt{a*b}.\\
    For instance, in a 3-letter word ``\texttt{abc}'', wildcards can be located at the letter locations (except for the \texttt{a}), or between all letters, giving us $2^{2+2} = 2^{2(3-1)} = 2^4 = 16$ possibilities. The wildcards will be added in the following form: ``\texttt{abc}'' $\rightarrow$ \texttt{a.b.c}, \texttt{a*b.c}, \texttt{a.*.c}, \texttt{a.b*c}, \texttt{a.b.*}, \texttt{a.b**}, \texttt{a.*.*}, \texttt{a*b.*}, \texttt{a**.*}, \texttt{a*b**}, \texttt{a**.c}, \texttt{a.**c}, \texttt{a.***}, \texttt{a*b**}, \texttt{a****}, \texttt{a***c}.\footnote{The dot in the wildcard strings is just a seperation to have a better visual representation of the possibilities.}
    \item Generate all possible replacements for a term. We have an alphabet of 26 letters and 10 different digits with whom we can replace every single letter. But we hit the same wall as with the wildcards, only with a much higher set of possibilities ($36^{n}$ instead of $2^{2n-1}$). Figure \ref{fig:wa} shows that, no matter the value of $n$, this option will always yield more possibilities.
    \begin{figure}[h!]
        \centering
        \includegraphics[width=.75\textwidth]{images/plots.eps}
        \caption{WolframAlpha-made plot of both $y=36^{x}$ and $y=2^{2x-1}$.}
        \label{fig:wa}
    \end{figure}\\
    On top of that, if we say that we only replace one or two letters at a time, there are still unexplored options. Let's say we want to look for ``\texttt{code}'', but accidentally only type ``\texttt{cod}'', this method will not find the right documents.
    \item Lucene has this \texttt{SpellChecker} class that can suggest the $n$ closest words in the dictionary of all files. This method allows us to resolve typos and tweak the amount of possible replacements\footnote{It's illogical that each word should match all of its possibilities. From ``\texttt{bear}'' to ``\texttt{beard}'', ``\texttt{boar}'' or ``\texttt{beer}'' are smaller changes than replacing it with ``\texttt{glad}''.}. For our purposes, we assumed 5 similar words (and the current one, making it 6 checks) should do the trick. We merge them all together in one big \texttt{OR}-gate for each term.
\end{itemize}
As you can guess, we have decided on the latter option, which can be implemented as follows (assume \texttt{term} is our term):
\begin{lstlisting}[language=Java]
// Did you mean? https://www.javacodegeeks.com/2010/05/did-you-mean-feature-lucene-spell.html
SpellChecker spellchecker = new SpellChecker(spellIndexDirectory);
// To index a field of a user index:
spellchecker.indexDictionary(new LuceneDictionary(my_lucene_reader, a_field));
// To index a file containing words:
spellchecker.indexDictionary(new PlainTextDictionary(
    new File("myfile.txt")));
String[] suggestions = spellchecker.suggestSimilar(term, 5);
String newTerm = "(" + String.join(" OR ", suggestions) + ")";
\end{lstlisting}
% https://lucene.apache.org/core/6_0_1/suggest/org/apache/lucene/search/spell/SpellChecker.html
% Mss: https://www.inetsoftware.de/documentation/other-products/jortho/apispec/

Combined with \textsf{String Separation}, we get:
\begin{lstlisting}[language=Java]
SpellChecker spellchecker = new SpellChecker(spellIndexDirectory);
spellchecker.indexDictionary(new LuceneDictionary(my_lucene_reader, a_field));
spellchecker.indexDictionary(new PlainTextDictionary(
    new File("myfile.txt")));
String[] terms = word.split(" ");
ArrayList<String> newTerms = new ArrayList<>();
for(String term: terms) {
    // Just in case there are multiple spaces
    if(term.isEmpty()) { continue; }
    
    List<String> suggestions = new ArrayList<>(Arrays.asList(spellchecker.suggestSimilar(term, 5));
    
    // Make sure the actual term is part of the query
    if(!suggestions.contains(term)) {
        suggestions.add(term);
    }
    newTerms.add("(" + String.join(" OR ", suggestions) + ")")
}
String newWord = String.join(" AND ", newTerms);
\end{lstlisting}

\section{Benchmark Study}\label{sec:bstudy}
To evaluate the retrieval performance of Lucene, the PR- and the ROC-curve will do great at visualizing this performance. To plot these curves, the Precision ($P$), Recall ($R = TPR$) and Fallout ($F = FPR$) need to be computed, which will be realized by \textit{manual labeling}. Formulas for these numbers are given as follows:
\begin{align*}
    P &= \frac{TP}{TP+FP}&
    R &= \frac{TP}{TP+FN}&
    F &= \frac{FP}{FP+TN}
\end{align*}

Unfortunately, for our use-case, the ROC (and AUC) are not the best measures to use, seeing as they only show the quality of the order compared to a random order. In this document retrieval system, the top-$k$ is the most important part of the returned documents. To measure the quality of this top list, we will calculate the \textsf{Precision@k}, \textsf{Recall@k} and \textsf{Accuracy@k} values. Formulas for these numbers are given as follows, using $R$ is the total relevant documents with $r \subseteq R$ documents in the top-$k$ and $t$ are the TN documents in that top-$k$:

\begin{align*}
P@k &= \frac{r}{k}&
R@k &= \frac{r}{R}&
A@k &= \frac{r}{r+t}
\end{align*}

\subsection{Manual Labeling}
First, we randomly choose 500 documents out of the complete data set. Then, 35 different queries are manually created based on the content of these documents. The queries are `human real life' created such that the use case is matching the reality. Some examples of the queries are: \textit{``How to read and write to a file''} and \textit{``Beautifulsoup web scrapers''}. Next to these queries, there are also two test queries to check if the algorithm works, namely the queries \textit{``Python''} and \textit{``C++''}.

Each document is deeply looked into and manually mapped to which query/queries should linked to this document.

\subsection{Performance Scoring}
And then comes the scoring phase. For all of these queries, we will determine the scores of all documents. This gives us two general information files.

The first one is a results file that states for each question how good it performed on which queries. Secondly, there is a relevance file where each query maps to a list of question ids. These ids represent the questions that were marked as relevant/correct document for the corresponding query.

A \textsf{Hapax Legomenon} (or a \textsf{hapax} for short) is a word or a concept that only occurs only once in a given context (see \cite{hapax}). Within our context, we can state that a query could a \textsf{hapax} if there is only one document that uses it and will match. Or, in layman's terms, a query (or a set thereof) that is only used by a single question. These do not give an accurate representation of our dataset, so we made the queries such that it has at least 2 words. Similarly, we believe that a query will only match to only one document is not an accurate choice, so we removed these queries. As a result we only have queries that will match to at least 2 documents.

\subsubsection{PR and ROC}
For each manual created query, we can now determine the top-$k$ precision, recall and fallout (as is shown in the lecture slides), which allows us to create the Precision-Recall (PR) Curve and the ROC-Curve for each query.

Because this will yield a lot of curves, most of which without any significant features, we have decided to represent the PR and ROC as an upper bound (i.e. the curve that is generally higher than all values). Note that this may give a skewed view of our results, but it seemed the most doable option. We tried using (weighed) averages, minimals, differences..., but unfortunately all of these resulted in plots with strange artifacts. An example of such a plot is shown in figure \ref{fig:artifact}.

As described in section \ref{sec:bstudy} the ROC and AUC are not the best measures for the performance analyses in our use-case, but it is still interesting enough to have a look at the over all scoring.

\begin{figure}[htp]
	\centering
	\includegraphics[width=\textwidth]{images/artifact}
	\caption{PR/ROC plots with artifacts due to maximization}
	\label{fig:artifact}
\end{figure}

\subsection{\textsf{Precision@k} and \textsf{Recall@k}}
For document retrieval, the top-$k$ returned documents are really interesting. Usually, a user types in the query and they only look at the first 10, 20 results (read: the top-$k$). So the top results are really important and should (ideally) all consist out of relevant documents. The \textsf{precision@k} is a way of measuring how good the top-$k$ is. It calculates the percentage of the relevant documents in the top listed documents.

Beside, the \textsf{recall@k} is also calculated: how many documents are there in the top-$k$ out of the relevant, correct documents. Unfortunately it doesn't give that much extra information for the performance of our system (similar for the \textsf{accuracy@k}), but it is calculated for its completeness. 

\subsubsection{Initial Score}
Using the \texttt{StandardAnalyzer} of Lucene\footnote{Which does a lot behind the scenes, including (but not limited to) removing stopwords and lowercase transformations}, the scores are computed as described above. This gives us an idea how well Lucene initially performs.

Using the averaged PR- and ROC-curves to visualize the results, we can conclude from Figure \ref{fig:initBP}, Lucene is doing a good job for the task at hand.

\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{images/2ndTerm/Standard}
    \caption{Results initial benchmark performance.}
    \label{fig:initBP}
\end{figure}

Next to these curves, the \textsf{Precision@k} and \textsf{Recall@k} are even more important to have an idea about the top results. Note: for the entire project, we have taken $k=20$.

\begin{table}[htp]
	\centering
	\begin{tabular}{|r|l|}
		\hline
		Weighted \textsf{Precision@20} & 0.327056277056277 \\ \hline
		Weighted \textsf{Recall@20} & 0.24085364440294335 \\ \hline
		Weighted \textsf{Accuracy@20} & 0.6367911543988225 \\ \hline
	\end{tabular}
	\caption{Results top-20 for the initial benchmark performance.}
	\label{tbl:atk-init}
\end{table}

\subsection{Scoring Models}\label{sec:scmodels}
To check whether the scoring models have any influence on the performance, the different \texttt{Similarity} measures have been used: \texttt{TFIDFSimilarity}\footnote{\texttt{org.apache.lucene.search.similarities.TFIDFSimilarity}} and \texttt{BooleanSimilarity}\footnote{\texttt{org.apache.lucene.search.similarities.BooleanSimilarity}}. 

Different similarity measurements have an impact on the performance of the system. The results give an overview of the boolean similairty where the terms score is equal to their query boost. The other measurement is the Tf-Idf similiartity; this scores the documents based on the cosine similarity in a VSM as described in section \ref{sec:ss}. The Tf-Idf and the Boolean similarity have exactly the same curves as the initial benchmark (see figure \ref{fig:initBP} and the top-$k$ results in table \ref{tbl:atk-init}). We have chosen to continue the project with the Tf-Idf similiarity measure.

\section{Optimizations}\label{sec:optimizations}
As was clear from figure \ref{fig:initBP} and table \ref{tbl:atk-init}, we can do better. This is why we will try to optimize our algorithms, in the hope of obtaining better results. In order to achieve this goal, we will reimplement the \texttt{StandardAnalyzer} as a basis for our \texttt{MyCustomAnalyzer} and work from there.

\subsection{Removing Special Characters and Whitespace}
As discussed in depth in section \ref{sec:query}, we will transform our fields so all specialities are discarded. If we base ourselves on the \texttt{StandardAnalyzer} and on the transformations we want to apply, as per section \ref{sec:query}, all that's left to do in order to have a valid transformation is to remove all non-alphanumerical characters and additional whitespace from our text.

This is done via adding a \texttt{PatternReplaceFilter}\footnote{\texttt{org.apache.lucene.analysis.pattern.PatternReplaceFilter}} and a \texttt{TrimFilter}\footnote{\texttt{org.apache.lucene.analysis.miscellaneous.TrimFilter}} in our \texttt{Analyzer}.

In figure \ref{fig:alphanumeric}, you can find that there was no significant difference compared to the original plot (figure \ref{fig:initBP}) and to the \textsf{Precision@k} value.
\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{images/2ndTerm/Trim}
    \caption{Results Benchmark Performance with Initial Filters.}
    \label{fig:alphanumeric}
\end{figure}

\begin{table}[htp]
	\centering
	\begin{tabular}{|r|l|}
		\hline
		Weighted \textsf{Precision@20} & 0.327056277056277 \\ \hline
		Weighted \textsf{Recall@20} & 0.24085364440294335 \\ \hline
		Weighted \textsf{Accuracy@20} & 0.6367911543988225 \\ \hline
	\end{tabular}
	\caption{Results top-20 with Initial Filters.}
	\label{tbl:atk-trim}
\end{table}

\subsubsection{Word Delimiter}\label{sec:word-delimiter}
The \texttt{StandardTokenizer} will be extended with a \texttt{WordDelimiterGraphFilter}\footnote{\texttt{org.apache.lucene.analysis.miscellaneous.WordDelimiterGraphFilter}}. This will split the words into multiple subwords and performs optional transformations on subword groups. \cite{lucene}

For example, if someone wants to search for ``\texttt{Macbook}'', but the user types ``\texttt{Mac Book}''. The WordDelimiter will catch this query because the word ``\texttt{Macbook}'' will internally saved as \{``\texttt{Mac}'', ``\texttt{Book}'', ``\texttt{Macbook}''\}. This way, we can cope with different cases of compound words.

At first, it sounds like a good idea, but, unfortunately, it doesn't have a huge impact on the performance. It even results in the same PR- and ROC-cuves as Figure \ref{fig:alphanumeric} and the same for Table \ref{tbl:atk-trim}! We believe this is because this kind of filter does not apply for our current set of queries.

\subsubsection{Stemming}\label{sec:stemming}
Next, stemming\footnote{\texttt{org.apache.lucene.analysis.PorterStemFilter}} has been added to the analyzer such that multiple variants of a word are mapped to the same word (i.e. plurals are mapped to its singular). There is a performance increase on this front, as you may be able to deduce from figure \ref{fig:stemming}. (Yes, it looks once more like the very same plot, possibly because of the way we generate the plots.) More importantly there is a positive impact on the Precision@k, which is increased after applying the stemming (Table \ref{tbl:atk-p}).

\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{images/2ndTerm/Porter+Trim+WordDelimiter}
    \caption{Results Analyzer extended with Stemming}
    \label{fig:stemming}
\end{figure}

\begin{table}[htp]
	\centering
	\begin{tabular}{|r|l|}
		\hline
		Weighted \textsf{Precision@20} & 0.3775088547815821 \\ \hline
		Weighted \textsf{Recall@20} & 0.22292070811981587 \\ \hline
		Weighted \textsf{Accuracy@20} & 0.6506418868487834 \\ \hline
	\end{tabular}
	\caption{Results top-20 Analyzer extended with Stemming.}
	\label{tbl:atk-p}
\end{table}

\subsubsection{$n$-grams}\label{sec:ngrams}
Now, let's see what happens if we divide the words into tuples of length $n$. This is called $n$-gram filtering\footnote{\texttt{org.apache.lucene.analysis.ngram.NGramTokenFilter}}. We tried 7 different values for $n = \{2, 3, 4, 5, 6, 7, 8\}$.

For all $n$, the ROC curves are all different and some have a good opportunity to be extended in our custom analyzer. Especially $n=8$ has a potential ROC curve for all returned documents. Beside, the \textsf{Precision@k} has the conclusion - which is more important to be considered for the custom analyzer. For the top-$k$ results the \textsf{Precision@k} has increased a lot, namely to almost $0.42$. Because of this huge improvement, we will include the $n$-grams (for $n=8$) in our optimizations.

\begin{figure}[htp]
	\centering
	\includegraphics[width=\textwidth]{images/2ndTerm/ExclSyn_Ngram8}
	\caption{Results Analyzer extended with $n$-grams (for $n=8$)}
	\label{fig:ngram}
\end{figure}

\begin{table}[htp]
	\centering
	\begin{tabular}{|r|l|}
		\hline
		Weighted \textsf{Precision@20} & 0.417955060812204 \\ \hline
		Weighted \textsf{Recall@20} & 0.233950930066487 \\ \hline
		Weighted \textsf{Accuracy@20} & 0.680485720858392 \\ \hline
	\end{tabular}
	\caption{Results top-20 Analyzer extended with $n$-grams (for $n=8$).}
	\label{tbl:atk-n}
\end{table}

\subsection{Manual Thesaurus-Based Query Expansion}\label{sec:synonyms}
A thesaurus is a list of synonyms. For each term in our dictionary, we may create such a list to associate the words with. Theoretically, this should increase our general recall, but may decrease precision, especially when we're talking about homonyms and such.

As a last improvement, we create such a filter\footnote{\texttt{org.apache.lucene.analysis.synonym.SynonymFilter}} and add it to the analyzer. This way, for each word we see, a set of synonyms will be associated with it, using a \texttt{SynonymMap}\footnote{\texttt{org.apache.lucene.analysis.synonym.SynonymMap}}. Use used an offline dictionary of thesaurus, \cite{synonyms}, which is built with approximately $169\,000$ English words and is two-directional.

To our surprise there was an enormous decrease in performance (approximately 12.7\% in terms of \textsf{Precision@k}), meaning that the synonyms get in the way of the analyzer and scores only worse. This  is shown in figure \ref{fig:synonyms} and table \ref{tbl:atk-ns}. That's why we decided to not extend the analyzer with the synonym filter.
\begin{figure}[htp]
	\centering
	\includegraphics[width=\textwidth]{images/2ndTerm/ExclNgram_afterSynonym}
	\caption{An overview of the extended Analyzer for manual thesaurus-based query expansion.}
	\label{fig:synonyms}
\end{figure}

\begin{table}[htp]
	\centering
	\begin{tabular}{|r|l|}
		\hline
		Weighted \textsf{Precision@20} & 0.29145803456949 \\ \hline
		Weighted \textsf{Recall@20} & 0.41720735195784 \\ \hline
		Weighted \textsf{Accuracy@20} & 0.505901510762118 \\ \hline
	\end{tabular}
	\caption{Results top-20 Analyzer extended with manual thesaurus-based query expansion.}
	\label{tbl:atk-ns}
\end{table}


\subsection{Final Results}
A final result of our benchmarking study can be found in figure \ref{fig:ngram} and table \ref{tbl:atk-n}, where everything comes together: lower case conversion, trimming, removing of non-alphanumeric values, word delimiters, stemming, and $n$-grams (for $n=8$).

Note that manual thesaurus-based query expansion is not included in this result.

\subsubsection{Summary \textsf{Precision@k}}
The \textsf{Precision@k} values were decisive in creating the optimal custom analyzer. By adding the stemming, the results were better in the top-$k$. By combining it with the $n$-grams, the results were astonishing. 

To give an idea about the impact, we have put all the values next to each other in a histogram (figure \ref{fig:prk}). It is clearly to see that `C N7' and `C N8' are the best scoring custom analyzers (focusing at the \textsf{Precision@k}).

\begin{figure}[htp]
	\centering
	\includegraphics[width=\textwidth]{images/Hist-at-k}
	\caption{An overview of the impact to the custom analyzers of the \textsf{Precision@k} and \textsf{Recall@k}. Where the `S' stands for Standard Analyzer, the `C' for Custom, `P' for adding Porter stemming (also trimming and word delimiter), `S' for adding Synonyms, the `N' for $n$-grams (this is including Porter, excl. synonyms) and the final `C' is the total Custom with all the elements active.}
	\label{fig:prk}
\end{figure}

\newpage
\section{Rocchio Algorithm}\label{sec:rocchio}
One possible optimization can come from the \textsf{Rocchio} algorithm for pseudo-relevance feedback. It's principle is simple: each document is located in an $n$-dimensional space, we search for a query (in the same space), mark some documents as ``\textit{relevant}'' and change the angle of our query vector so it is located in the center of the cluster of relevant documents. Note that this method assumes all relevant documents are located in the same cluster.

To apply the algorithm, the following formula can be used to alter our original query $\overrightarrow{Q_o}$:
$$\overrightarrow{Q_n} = \alpha \overrightarrow{Q_o} + \beta \frac{1}{|D_R|} \sum_{\overrightarrow{Q_j} \in D_R}{\overrightarrow{Q_j}} + \gamma \frac{1}{|D_{NR}|} \sum_{\overrightarrow{Q_k} \in D_{NR}}{\overrightarrow{Q_k}}$$
In section \ref{sec:bstudy}, we described a way to easily and uniformly determine if a document was ``\textit{correct}'' (read: \textit{relevant}) or not. So we have a query $q$, a set of relevant documents $D_R$ and a set of irrelevant documents $D_{NR}$ (which can be obtained from the simple truth that $D_R + D_{NR} = D$, where $D$ represents the set of all documents). Because we remembered the frequency vectors (see section \ref{sec:fv}), we can easily transform all these variables into their corresponding vectors and apply the formula.

Unfortunately, while we tried to come up with a viable algorithm to compute $\overrightarrow{Q_n}$ from the information Lucene provided on the query, we failed in doing so. Instead, we pointed our attention to already-existing library for computing pseudo-relevance feedback \cite{rocchio} and its dependency \cite{glis}. The last activity in either of these repositories dates from 2017, hence a lot of functions and features follow an outdated Lucene version, compared to our project. Therefore, instead of adding these repositories as submodules, they were added by copying the files. This way, we could update the dependencies to work with our code. Additionally, we changed our build system from a single Java-command to Maven.

Exercising \textsf{Rocchio} for the \texttt{StandardAnalyzer} yields the results shown in figure \ref{fig:rocchio-standard}. Compared to figure \ref{fig:initBP}, we can see clearly that the AUC is increased, resulting in a better ROC curve. However, the \textsf{Precision@k} value is decreased by approximately $16.4\%$ which isn't a good result.  Therefore, the \textsf{Rocchio} algorithm combined with the standard analyzer performs worse than when using our custom analyzer.

\begin{figure}[htp]
	\centering
	\includegraphics[width=\textwidth]{images/rocchio-standard}
	\caption{The standard analyzer combined with the \textsf{Rocchio} algorithm for pseudo-relevance feedback.}
	\label{fig:rocchio-standard}
\end{figure}

\begin{table}[htp]
	\centering
	\begin{tabular}{|r|l|}
		\hline
		Weighted \textsf{Precision@20} & 0.2542857142857142 \\ \hline
		Weighted \textsf{Recall@20} & 0.6373501077545152 \\ \hline
		Weighted \textsf{Accuracy@20} & 0.0.10271136159409343 \\ \hline
	\end{tabular}
	\caption{Results top-20 standard analyzer combined with the \textsf{Rocchio} algorithm for pseudo-relevance feedback.}
	\label{tbl:atk-rocc}
\end{table}

\subsection{Optimizations}
As described above, the \textsf{Rocchio} algorithm is parameterized with an $\alpha$, $\beta$ and $\gamma$ value. In the implemented algorithm of the used library \cite{rocchio}, there are only 2 parameters for \textsf{Rocchio} ($\alpha$ and $\beta$) and 2 other parameters for the BM25-similarity measure. We have used a Gridsearch to find the optimal $\alpha$ and $\beta$. To visualize this, the precision@k, recall@k and accuracy@k values are showed in a heatmap for the different parameter values.

\begin{figure}[htp]
	\centering
	\subfloat[Precision@k]{\label{a}\includegraphics[width=.5\linewidth]{images/Heatmap_pr}}\hfill
	\subfloat[Recall@k]{\label{b}\includegraphics[width=.5\linewidth]{images/Heatmap_rec}}\par 
	\subfloat[Accuracy@k]{\label{c}\includegraphics[width=.5\linewidth]{images/Heatmap_acc}}
	\caption{Heatmap}
	\label{fig:heatmap}
\end{figure}

We mentioned that for this use case - with the 500 manually labeled documents - the best parameters must be set to $\alpha = 1.0$ and $\beta = 0.0$. The relevance feedback algorithm isn't working in its ideal environment, because of the small use case. Although, for a bigger use case - with lots of more documents - the \textsf{Rocchio} algorithm will perform much better. As a result, the $\beta$ parameter will also have an impact to the document retrieval (in contrast to our use case). Unfortunately, the optimal Rocchio parameters will also perform with an extreme worse \textsf{Precision@k} namely equal to $0.15428$. 

\clearpage
\bibliographystyle{abbrv}
\bibliography{mybib}

\end{document}
