<?xml version="1.0" encoding="utf-8"?>
<qroot>
<question>
<Title>How to make a fast dictionary that contains another dictionary?</Title>
<Body>&lt;p&gt;I have a &lt;code&gt;map&amp;lt;size_t, set&amp;lt;size_t&amp;gt;&amp;gt;&lt;/code&gt;, which, for better performance, I&#x27;m actually representing as a lexicographically-sorted &lt;code&gt;vector&amp;lt;pair&amp;lt;size_t, vector&amp;lt;size_t&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;What I need is a &lt;code&gt;set&amp;lt;T&amp;gt;&lt;/code&gt; with &lt;em&gt;fast&lt;/em&gt; insertion times (removal doesn&#x27;t matter), where &lt;code&gt;T&lt;/code&gt; is the data type above, so that I can check for duplicates (my program runs until there are no more unique &lt;code&gt;T&lt;/code&gt;&#x27;s being generated.).&lt;/p&gt;&lt;p&gt;So far, switching from &lt;code&gt;set&lt;/code&gt; to &lt;code&gt;unordered_set&lt;/code&gt; has turned out to be quite beneficial (it makes my program run &gt; 25% faster), but even now, inserting &lt;code&gt;T&lt;/code&gt; still seems to be one of the main bottlenecks.&lt;/p&gt;&lt;p&gt;The maximum number of integers in a given &lt;code&gt;T&lt;/code&gt; is around ~1000, and each integer is also &amp;lt;= ~1000, so the numbers are quite small (but there are thousands of these &lt;code&gt;T&lt;/code&gt;&#x27;s being generated).&lt;/p&gt;&lt;p&gt;What I have already tried:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Using &lt;code&gt;unsigned short&lt;/code&gt;. It actually decreases performance slightly.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Using Google&#x27;s &lt;a href=&quot;https://code.google.com/p/cpp-btree/source/browse/btree_map.h&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;btree::btree_map&lt;/code&gt;&lt;/a&gt;.&lt;br&gt;It&#x27;s actually much slower because I have to work around the iterator invalidation.&lt;br&gt;(I have to copy the keys, and I think that&#x27;s why it turned out slow. It was at least twice as slow.)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Using a different hash function. I haven&#x27;t found any measurable difference as long as I use something reasonable, so it seems like this can&#x27;t be improved.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;What I have &lt;em&gt;not&lt;/em&gt; tried:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Storing &quot;fingerprints&quot;/hashes instead of the actual sets.&lt;br&gt;This sounds like the perfect solution, except that the fingerprinting function needs to be &lt;em&gt;fast&lt;/em&gt;, and I need to be &lt;em&gt;extremely&lt;/em&gt; confident that collisions won&#x27;t happen, or they&#x27;ll screw up my program.&lt;br&gt;(It&#x27;s a deterministic program that needs exact results; collisions render it useless.)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Storing the data in some other compact, CPU-friendly way.&lt;br&gt;I&#x27;m not sure how beneficial this would be, because it might involve copying around data, and most of the performance I&#x27;ve gained so far is by (cleverly) avoiding copying data in many situations.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;What else can I do to improve the speed, if anything?&lt;/h3&gt;</Body>
<Tags>c++,performance,data-structures,set,hashtable</Tags>
</question>
<answer>
<Body>&lt;p&gt;Don&#x27;t be afraid of collisions, use cryptographic hash. But choose a fast one. 256 bit collision is MUCH less probable than hardware error. Sun used it to deduplicate blocks in ZFS. ZFS uses SHA256. Probably you can use less secure hash. If it takes $1000000 to find one collision hash isn&#x27;t secure but one collision don&#x27;t seem to drop your performance. Many collisions would cost many $1000000. You can use something like (unordered) &lt;code&gt;multimap&amp;lt;SHA, T&amp;gt;&lt;/code&gt; to deal with collisions. By the way, ANY hash table suffer from collisions (or takes too many memory), so ordered map (rbtree in gcc) or btree_map has better guaranteed time. Also hash table can be DOSed via hash collisions. Probably a secret salt can solve this problem. It is due to table size is much less than number of possible hashes.&lt;/p&gt;</Body>
</answer>
<answer>
<Body>&lt;p&gt;I am under the impression that you have 3 different problems here:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;you need the &lt;code&gt;T&lt;/code&gt; itself to be relatively compact and easy to move around&lt;/li&gt;&lt;li&gt;you need to quickly check whether a &lt;code&gt;T&lt;/code&gt; is a possible duplicate of an already existing one&lt;/li&gt;&lt;li&gt;you finally need to quickly insert the new &lt;code&gt;T&lt;/code&gt; in whatever data structure you have to check for duplicates&lt;/li&gt;&lt;/ul&gt;&lt;hr&gt;&lt;p&gt;Regarding &lt;code&gt;T&lt;/code&gt; itself, it is not yet as compact as it could be. You could probably use a single &lt;code&gt;std::vector&amp;lt;size_t&amp;gt;&lt;/code&gt; to represent it:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;N pairs&lt;/li&gt;&lt;li&gt;N Indexes&lt;/li&gt;&lt;li&gt;N &quot;Ids&quot; of I elements each&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;all that can be linearized:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;[N, I(0), ..., I(N-1),    R(0) = Size(Id(0)), Id(0, 0), ... , Id(0, R(0)-1),    R(1) = ... ]&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and this way you have a single chunk of memory.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Note: depending on the access pattern you may have to tweak it, specifically if you need random access to any ID.&lt;/em&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Regarding the possibility of duplicates, a hash-map seems indeed quite appropriate. You will need a good hash function, but with a single array of &lt;code&gt;size_t&lt;/code&gt; (or &lt;code&gt;unsigned short&lt;/code&gt; if you can, it is smaller), you can just pick MurmurHash or CityHash or SipHash. They all are blazing fast and do their damnest to produce good quality hash (not cryptographic ones, emphasis is on speed).&lt;/p&gt;&lt;p&gt;Now, the question is &lt;em&gt;when&lt;/em&gt; is it slow when checking for duplicates.&lt;/p&gt;&lt;p&gt;If you spend too much time checking for non-existing duplicates because the hash-map is too big, you might want to invest in a Bloom Filter in front of it.&lt;/p&gt;&lt;p&gt;Otherwise, check your hash function to make sure that it is really fast and has a low collision rate and your hash-map implementation to make sure it only ever computes the hash once.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Regarding insertion speed. Normally a hash-map, specifically if well-balanced and pre-sized, should have one of the quickest insertion. Make sure you move data into it and do not copy it; if you cannot move, it might be worth using a &lt;code&gt;shared_ptr&lt;/code&gt; to limit the cost of copying.&lt;/p&gt;</Body>
</answer>
<answer>
<Body>&lt;p&gt;You can also:1) use short ints2) reinterpret your array as an array of something like uint64_t for fast comparison (+some trailing elements), or even reinterpret it as an array of 128-bit values (or 256-bit, depending on your CPU) and compare via SSE. This should push your performance to memory speed limit.From my experience SSE works fast with aligned memory access only. uint64_t comparison probably needs alignment for speed too, so you have to allocate memory manually with proper alignment (allocate more and skip first bytes). tcmalloc is 16 byte-aligned, uint64_t-ready. It is strange that you have to copy keys in btree, you can avoid it using shared_ptr. With fast comparisons and slow hash btree or std::map may turn out to be faster than hash table. I guess any hash is slower than memory. You can also calculate hash via SSE and probably find a library that does it.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;PS I strongly recommend you to use profiler if you don&#x27;t yet. Please tell % of time your program spend to insert, compare in insert and calculate hash.&lt;/p&gt;</Body>
</answer>
</qroot>
