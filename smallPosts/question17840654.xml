<?xml version="1.0" encoding="utf-8"?>
<qroot>
<question>
<Title>Scrapy just crawls one page and not the other pages</Title>
<Body>&lt;p&gt;I am making a Project on online business directory using django and scrapy.I am just using scrapy to scrap data and store it into the database.the django part is not directly connected to scrapy.I need to scrap the business details from a business directory &lt;a href=&quot;http://directory.thesun.co.uk/find/uk/computer-repair&quot; rel=&quot;nofollow&quot;&gt;http://directory.thesun.co.uk/find/uk/computer-repair&lt;/a&gt;  ..The problem is that it is just fecting details from only one page and not from others..please help to solve this problem..&lt;/p&gt;&lt;p&gt;Please see my code and help me to solve it&lt;/p&gt;&lt;p&gt;spider code::&lt;/p&gt;&lt;pre&gt;&lt;code&gt;from scrapy.spider import BaseSpiderfrom scrapy.selector import HtmlXPathSelectorfrom project2.items import Project2Itemclass ProjectSpider(BaseSpider):    name = &quot;project2spider&quot;    allowed_domains = [&quot;http://directory.thesun.co.uk/&quot;]    start_urls = [        &#x27;http://directory.thesun.co.uk/find/uk/computer-repair&#x27;,        &#x27;http://directory.thesun.co.uk/find/uk/computer-repair/page/2&#x27;,        &#x27;http://directory.thesun.co.uk/find/uk/computer-repair/page/3&#x27;,        &#x27;http://directory.thesun.co.uk/find/uk/computer-repair/page/4&#x27;,        &#x27;http://directory.thesun.co.uk/find/uk/computer-repair/page/5&#x27;,        &#x27;http://directory.thesun.co.uk/find/uk/computer-repair/page/6&#x27;,        &#x27;http://directory.thesun.co.uk/find/uk/computer-repair/page/7&#x27;,        &#x27;http://directory.thesun.co.uk/find/uk/computer-repair/page/8&#x27;,        &#x27;http://directory.thesun.co.uk/find/uk/computer-repair/page/9&#x27;,        &#x27;http://directory.thesun.co.uk/find/uk/computer-repair/page/10&#x27;        ]    def parse(self, response):        hxs = HtmlXPathSelector(response)        sites = hxs.select(&#x27;//div[@class=&quot;abTbl &quot;]&#x27;)        items = []        for site in sites:            item = Project2Item()            item[&#x27;Catogory&#x27;] = site.select(&#x27;span[@class=&quot;icListBusType&quot;]/text()&#x27;).extract()            item[&#x27;Bussiness_name&#x27;] = site.select(&#x27;a/@title&#x27;).extract()            item[&#x27;Description&#x27;] = site.select(&#x27;span[last()]/text()&#x27;).extract()            item[&#x27;Number&#x27;] = site.select(&#x27;span[@class=&quot;searchInfoLabel&quot;]/span/@id&#x27;).extract()            item[&#x27;Web_url&#x27;] = site.select(&#x27;span[@class=&quot;searchInfoLabel&quot;]/a/@href&#x27;).extract()            item[&#x27;adress_name&#x27;] = site.select(&#x27;span[@class=&quot;searchInfoLabel&quot;]/span/text()&#x27;).extract()            item[&#x27;Photo_name&#x27;] = site.select(&#x27;img/@alt&#x27;).extract()            item[&#x27;Photo_path&#x27;] = site.select(&#x27;img/@src&#x27;).extract()            items.append(item)        return items&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;My items.py code is as follows:: &lt;/p&gt;&lt;pre&gt;&lt;code&gt;from scrapy.item import Item, Fieldclass Project2Item(Item):    Catogory = Field()    Bussiness_name = Field()    Description = Field()    Number = Field()    Web_url = Field()    adress_name = Field()    Photo_name = Field()    Photo_path = Field()&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;my settings.py is:::&lt;/p&gt;&lt;pre&gt;&lt;code&gt;BOT_NAME = &#x27;project2&#x27;SPIDER_MODULES = [&#x27;project2.spiders&#x27;]NEWSPIDER_MODULE = &#x27;project2.spiders&#x27;ITEM_PIPELINES = (    &#x27;project2.pipelines.MySQLStorePipeline&#x27;,)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;My Pipeline code is::&lt;/p&gt;&lt;pre&gt;&lt;code&gt;from scrapy import log#from scrapy.core.exceptions import DropItemfrom twisted.enterprise import adbapiimport MySQLdb.cursorsclass MySQLStorePipeline(object):    def __init__(self):        #  hardcoded db settings        self.dbpool = adbapi.ConnectionPool(&#x27;MySQLdb&#x27;,                db=&#x27;project&#x27;,                user=&#x27;root&#x27;,                passwd=&#x27;&#x27;,                host=&#x27;127.0.0.1&#x27;,                port=&#x27;3306&#x27;,                                            cursorclass=MySQLdb.cursors.DictCursor,                charset=&#x27;utf8&#x27;,                use_unicode=True            )def process_item(self, item, spider):    # run db query in thread pool    query = self.dbpool.runInteraction(self._conditional_insert, item)    query.addErrback(self.handle_error)    return itemdef _conditional_insert(self, tx, item):    insert_id = tx.execute(\        &quot;insert into crawlerapp_directory (Catogory, Bussiness_name, Description, Number, Web_url) &quot;        &quot;values (%s, %s, %s, %s, %s)&quot;,        (item[&#x27;Catogory&#x27;][0],         item[&#x27;Bussiness_name&#x27;][0],         item[&#x27;Description&#x27;][0],         item[&#x27;Number&#x27;][0],         item[&#x27;Web_url&#x27;][0],         )        )    tx.execute(\        &quot;insert into crawlerapp_adress (directory_id, adress_name) &quot;        &quot;values (%s, %s)&quot;,        (insert_id,         item[&#x27;adress_name&#x27;][0]         )        )    tx.execute(\        &quot;insert into crawlerapp_photos (directory_id, Photo_path, Photo_name) &quot;        &quot;values (%s, %s, %s)&quot;,        (insert_id,         item[&#x27;Photo_path&#x27;][0],         item[&#x27;Photo_name&#x27;][0]         )        )    log.msg(&quot;Item stored in db: %s&quot; % item, level=log.DEBUG)def handle_error(self, e):    log.err(e)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Please help me to scrap data from other pages too..&lt;/p&gt;</Body>
<Tags>python,django,web-scraping,screen-scraping,scrapy</Tags>
</question>
</qroot>
