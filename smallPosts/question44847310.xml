<?xml version="1.0" encoding="utf-8"?>
<qroot>
<question>
<Title>Why is file processing in python taking more time for chunks which come later in the file?</Title>
<Body>&lt;p&gt;I have a very simple code which parses a JSON file. The file contains each line as a JSON object. For some reason, the processing time for each row increases as I run the code.&lt;/p&gt;&lt;p&gt;Can someone explain to me why this is happening and how to stop this?&lt;/p&gt;&lt;p&gt;Here is the code snippet:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;from ast import literal_eval as leimport reimport stringfrom pandas import DataFrameimport pandasimport timef = open(&#x27;file.json&#x27;)df = DataFrame(columns=(column_names))row_num = 0while True:    t = time.time()    for line in f:        line = line.strip()        d = le(line)        df.loc[row_num] = [d[column_name1], d[column_name2]]        row_num+=1        if(row_num%5000 == 0):            print row_num, &#x27;done&#x27;, time.time() - t            breakdf.to_csv(&#x27;MetaAnalysis&#x27;, encoding=&#x27;utf-8&#x27;)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Part of the output is as follows:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;5000 done 11.4549999237     10000 done 16.5380001068    15000 done 24.2339999676    20000 done 36.3680000305    25000 done 50.0610001087    30000 done 57.0130000114    35000 done 65.9800000191    40000 done 74.4649999142 &lt;/code&gt;&lt;/pre&gt;&lt;p&gt;As visible the time is increasing for each row.&lt;/p&gt;</Body>
<Tags>python</Tags>
</question>
<answer>
<Body>&lt;p&gt;You are monotonically increasing the data structure df.loc by inserting new elements in the line&lt;/p&gt;&lt;pre&gt;&lt;code&gt;df.loc[row_num] = [d[column_name1], d[column_name2]].&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The variable df.loc seems to be a dictionary (e.g. &lt;a href=&quot;https://chrisalbon.com/python/pandas_indexing_selecting.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;). Inserting into a python dictionary is getting slower the more elements it already holds. This has already been discussed in &lt;a href=&quot;https://stackoverflow.com/questions/16256913/improving-performance-of-very-large-dictionary-in-python&quot;&gt;this stackoverflow response&lt;/a&gt;. Hence, the increasing size of your dictionary will eventually slow down the inner code of the loop.&lt;/p&gt;</Body>
</answer>
<answer>
<Body>&lt;p&gt;So, based on the answer by mayercn and comment by Hugh Bowell, I was able to identify the issue with the code.I modified the code as follows to reduce the time by 1/12th (average). TL;DR: I appended rows to a list and later appended it to a final dataframe.&lt;/p&gt;&lt;pre&gt;&lt;code&gt;from ast import literal_eval as leimport reimport stringfrom pandas import DataFrameimport pandasimport timef = open(&#x27;Filename&#x27;)df = DataFrame(columns=cols)row_num = 0while True:    t = time.time()    l = []    for line in f:        line = line.strip()        bug = le(line)        l.append([values])        row_num+=1        if(row_num%5000 == 0):            print row_num, &#x27;done&#x27;, time.time() - t            df = df.append(pandas.DataFrame(l),ignore_index=True)            breakdf.to_csv(&#x27;File&#x27;, index=&#x27;id&#x27;, encoding=&#x27;utf-8&#x27;)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Output time:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;5000 done 0.99800014495810000 done 1.0180001258915000 done 1.0169999599520000 done 0.99900007247925000 done 1.0460000038130000 done 1.0920000076335000 done 1.0620000362440000 done 1.1430001258945000 done 1.0090000629450000 done 1.06600022316&lt;/code&gt;&lt;/pre&gt;</Body>
</answer>
<answer>
<Body>&lt;p&gt;Pandas is notoriously slow about appending rows - it maintains hierarchical indices on the data, and every time you append a row it has to update all the indices.&lt;/p&gt;&lt;p&gt;This means it is much faster to add a thousand rows (then update) instead of adding a row (and updating) a thousand times.&lt;/p&gt;&lt;p&gt;Sample code to follow; I am still downloading &lt;a href=&quot;http://www.csis.ysu.edu/~alazar/msr14data/datasets/mozilla.tar.gz&quot; rel=&quot;nofollow noreferrer&quot;&gt;the mozilla.tar.gz file (453 MB)&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; apparently the file I downloaded and extracted (&lt;code&gt;/dump/mozilla/mozall.bson&lt;/code&gt;, 890 MB) is a MongoDB dump in bson with TenGen extensions, containing 769k rows. For testing purposes I took the first 50k rows and re-saved as json (result is 54 MB - average line is about 1200 characters), then used Notepad++ to break it to one record per line.&lt;/p&gt;&lt;p&gt;Most of the complexity here is for reading the file in chunks:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;from itertools import isliceimport pandas as pdfrom time import timeLINES_PER_BLOCK = 5000# read object chunks from json filewith open(&quot;mozilla.json&quot;) as inf:    chunks = []    while True:        start = time()        block = list(islice(inf, LINES_PER_BLOCK))        if not block:            # reached EOF            break        json  = &quot;[&quot; + &quot;,&quot;.join(block) + &quot;]&quot;        chunk = pd.read_json(json, &quot;records&quot;)        chunks.append(chunk)        done = time()        print(LINES_PER_BLOCK * len(chunks), &quot;done&quot;, done - start)        start = done# now combine chunksstart = time()df = pd.concat(chunks)done = time()print(&quot;Concat done&quot;, done - start)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;which gives&lt;/p&gt;&lt;pre&gt;&lt;code&gt;5000 done 0.1229319572448730510000 done 0.1203484535217285215000 done 0.1223988533020019520000 done 0.1194241046905517625000 done 0.1228291988372802730000 done 0.1193168163299560535000 done 0.127870082855224640000 done 0.1223828792572021545000 done 0.1209673881530761750000 done 0.20111417770385742Concat done 0.04361534118652344&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;for a total time of 1.355s; if you don&#x27;t need to chunk the file, it simplifies to&lt;/p&gt;&lt;pre&gt;&lt;code&gt;import pandas as pdfrom time import timestart = time()with open(&quot;mozilla.json&quot;) as inf:    json = &quot;[&quot; + &quot;,&quot;.join(inf) + &quot;]&quot;df = pd.read_json(json, &quot;records&quot;)done = time()print(&quot;Total time&quot;, done - start)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;which gives&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Total time 1.247551441192627&lt;/code&gt;&lt;/pre&gt;</Body>
</answer>
</qroot>
