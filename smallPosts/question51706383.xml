<?xml version="1.0" encoding="utf-8"?>
<qroot>
<question>
<Title>Pyspark : removing special/numeric strings from array of string</Title>
<Body>&lt;p&gt;To keep it simple I have a df with the following schema:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;root |-- Event_Time: string (nullable = true) |-- tokens: array (nullable = true) |    |-- element: string (containsNull = true)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;some of the elements of &quot;tokens&quot; have number and special characters for example:&lt;/p&gt;&lt;pre&gt;&lt;code&gt; &quot;431883&quot;, &quot;r2b2&quot;, &quot;@refe98&quot;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Any way I can remove all those and keep only actuals words ? I want to do an LDA later and want to clean my data before.I tried &lt;code&gt;regexp_replace&lt;/code&gt;, &lt;code&gt;explode&lt;/code&gt;, &lt;code&gt;str.replace&lt;/code&gt; with no success maybe I didn&#x27;t use them correctly.Thanks&lt;/p&gt;&lt;p&gt;edit2: &lt;/p&gt;&lt;pre&gt;&lt;code&gt;df_2 = (df_1.select(explode(df_1.tokens).alias(&#x27;elements&#x27;))          .select(regexp_replace(&#x27;elements&#x27;,&#x27;\\w*\\d\\w**&#x27;,&quot;&quot;))      )&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This works only if the column in a string type, and with explode method I can explode an array into strings but there is not in the same row anymore... Anyone can improve on this?&lt;/p&gt;</Body>
<Tags>python,regex,string,apache-spark,pyspark</Tags>
</question>
<answer>
<Body>&lt;pre&gt;&lt;code&gt;from pyspark.sql.functions import *df = spark.createDataFrame([([&quot;@a&quot;, &quot;b&quot;, &quot;c&quot;],), ([],)], [&#x27;data&#x27;])df_1 = df.withColumn(&#x27;data_1&#x27;, concat_ws(&#x27;,&#x27;, &#x27;data&#x27;))df_1 = df_1.withColumn(&quot;data_2&quot;, regexp_replace(&#x27;data_1&#x27;, &quot;[&#x27;{@]&quot;,&quot;&quot;))#df_1.printSchema()df_1.show()+----------+------+------+|      data|data_1|data_2|+----------+------+------+|[@a, b, c]|@a,b,c| a,b,c||        []|      |      |+----------+------+------+&lt;/code&gt;&lt;/pre&gt;</Body>
</answer>
<answer>
<Body>&lt;p&gt;The solution I found is (as also stated by pault in comment section):&lt;/p&gt;&lt;p&gt;After explode on tokens, I groupBy and agg with collect list to get back the tokens in the format I want them.&lt;/p&gt;&lt;p&gt;here is the comment of pault:After the explode, you need to groupBy and aggregate with collect_list to get the values back into a single row. Assuming Event_Time is a unique key:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;df2 = df_1    .select(&quot;Event_Time&quot;, regexp_replace(explode(&quot;tokens&quot;), &quot;&amp;lt;your regex here&amp;gt;&quot;)            .alias(&quot;elements&quot;)).groupBy(&quot;Event_Time&quot;)    .agg(collect_list(&quot;elements&quot;).alias(&quot;tokens&quot;)) &lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Also, stated by paul which I didnt know, there is currently no way to iterate over an array in pyspark without using udf or rdd.&lt;/p&gt;</Body>
</answer>
</qroot>
