<?xml version="1.0" encoding="utf-8"?>
<qroot>
<question>
<Title>Am I setting my multithreading web scraper properly?</Title>
<Body>&lt;p&gt;I&#x27;m trying to improve the speed of my web scraper, and I have thousands of sites I need to get info from. I&#x27;m trying to get the ratings and number of ratings for sites in Google search webpages from Facebook and Yelp. I would just use an API normally, but because I have a huge list of sites to search for and time is of the essence, Facebook&#x27;s small request limits per hour make this not feasible to use their Graph API (I&#x27;ve tried...). My sites are all in Google search pages. What I have so far (I have provided 8 sample sites for reproducibility):&lt;/p&gt;&lt;pre&gt;&lt;code&gt;from multiprocessing.dummy import Poolimport requestsfrom bs4 import BeautifulSouppools = Pool(8) #My computer has 8 coresproxies = MY_PROXIES#How I set up my urls for requests on Google searches. #Since each item has a &quot;+&quot; in between in a Google search, I have to format #my urls to copy it.site_list = [&#x27;Golden Gate Bridge&#x27;, &#x27;Statue of Liberty&#x27;, &#x27;Empire State Building&#x27;, &#x27;Millennium Park&#x27;, &#x27;Gum Wall&#x27;, &#x27;The Alamo&#x27;, &#x27;National Art Gallery&#x27;, &#x27;The Bellagio Hotel&#x27;]urls = list(map(lambda x: &quot;+&quot;.join(x.split(&quot; &quot;)), site_list)def scrape_google(url_list):    info = []    for i in url_list:        reviews = {&#x27;FB Rating&#x27;: None,                   &#x27;FB Reviews&#x27;: None,                   &#x27;Yelp Rating&#x27;: None,                   &#x27;Yelp Reviews&#x27;: None}           request = requests.get(i, proxies=proxies, verify=False).text        search = BeautifulSoup(search, &#x27;lxml&#x27;)        results = search.find_all(&#x27;div&#x27;, {&#x27;class&#x27;: &#x27;s&#x27;}) #Where the ratings roughly are        for j in results:             if &#x27;Rating&#x27; in str(j.findChildren()) and &#x27;yelp&#x27; in str(j.findChildren()[1]):                reviews[&#x27;Yelp Rating&#x27;] = str(j.findChildren()).partition(&#x27;Rating&#x27;)[2].split()[1] #Had to brute-force get the ratings this way.                reviews[&#x27;Yelp Reviews&#x27;] = str(j.findChildren()).partition(&#x27;Rating&#x27;)[2].split()[3]            elif &#x27;Rating&#x27; in str(j.findChildren()) and &#x27;facebook&#x27; in str(j.findChildren()[1]):                reviews[&#x27;FB Rating&#x27;] = str(j.findChildren()).partition(&#x27;Rating&#x27;)[2].split()[1]                reviews[&#x27;FB Reviews&#x27;] = str(j.findChildren()).partition(&#x27;Rating&#x27;)[2].split()[3]    info.append(reviews)return inforesults = pools.map(scrape_google, urls)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;I tried something similar to this, but I think I&#x27;m getting way too many duplicated results. Will multithreading make this run more quickly? I did diagnostics on my code to see which parts took up the most time, and by far getting the requests was the rate-limiting factor.&lt;/p&gt;&lt;p&gt;EDIT: I just tried this out, and I get the following error:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Invalid URL &#x27;h&#x27;: No schema supplied. Perhaps you meant http://h?&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;I don&#x27;t understand what the problem is, because if I try my     scrape_google function without multithreading, it works just fine (albeit very very slowly), so url validity should not be an issue.&lt;/p&gt;</Body>
<Tags>python,web-scraping,google-search</Tags>
</question>
<answer>
<Body>&lt;p&gt;Yes, multithreading will probably make it run more quickly.&lt;/p&gt;&lt;p&gt;As a very rough rule of thumb, you can usually profitably make about 8-64 requests in parallel, as long as no more than 2-12 of them are to the same host. So, one dead-simple way to apply that is to just toss all of your requests into a &lt;code&gt;concurrent.futures.ThreadPoolExecutor&lt;/code&gt; with, say, 8 workers. &lt;/p&gt;&lt;p&gt;In fact, that&#x27;s &lt;a href=&quot;https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor-example&quot; rel=&quot;nofollow noreferrer&quot;&gt;the main example for &lt;code&gt;ThreadPoolExecutor&lt;/code&gt; in the docs&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;(By the way, the fact that your computer has 8 cores is irrelevant here. Your code isn&#x27;t CPU-bound, it&#x27;s I/O bound. If you do 12 requests in parallel, or even 500 of them, at any given moment, almost all of your threads are waiting on a &lt;code&gt;socket.recv&lt;/code&gt; or similar call somewhere, blocking until the server responds, so they aren&#x27;t using your CPU.)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;However:&lt;/p&gt;&lt;blockquote&gt;  &lt;p&gt;I think I&#x27;m getting way too many duplicated result&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Fixing this may help far more than threading. Although, of course, you can do both.&lt;/p&gt;&lt;p&gt;I have no idea what your issue is here from the limited information you provided, but there&#x27;s a pretty obvious workaround: Keep a set of everything you&#x27;ve seen so far. Whenever you get a new URL, if it&#x27;s already in the set, throw it away instead of queuing up a new request.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Finally:&lt;/p&gt;&lt;blockquote&gt;  &lt;p&gt;I would just use an API normally, but because I have a huge list of sites to search for and time is of the essence, Facebook&#x27;s small request limits per hour make this not feasible&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;If you&#x27;re trying to get around the rate limits for a major site, (a) you&#x27;re probably violating their T&amp;amp;C, and (b) you&#x27;re almost surely going to trigger some kind of detection and get yourself blocked.&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;In your edited question, you attempted to do this with &lt;code&gt;multiprocessing.dummy.Pool.map&lt;/code&gt;, which is fine—but you&#x27;re getting the arguments wrong.&lt;/p&gt;&lt;p&gt;Your function takes a list of urls and loops over them:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;def scrape_google(url_list):    # ...    for i in url_list:&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;But then you call it with a single URL at a time:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;results = pools.map(scrape_google, urls)&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This is similar to using the builtin &lt;code&gt;map&lt;/code&gt;, or a list comprehension:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;results = map(scrape_google, urls)results = [scrape_google(url) for url in urls]&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;What happens if you get a single URL instead of a list of them, but try to use it as a list? A string is a sequence of its characters, so you loop over the characters of the URL one by one, trying to download each character as if it were a URL.&lt;/p&gt;&lt;p&gt;So, you want to change your function, like this:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;def scrape_google(url):    reviews = # …    request = requests.get(url, proxies=proxies, verify=False).text    # …    return reviews&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now it takes a single URL, and returns a set of reviews for that URL. The &lt;code&gt;pools.map&lt;/code&gt; will call it with each URL, and give you back an iterable of reviews, one per URL.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;sub&gt;1. Or maybe something more creative. Someone posted a question on SO a few years ago about a site that apparently sent corrupted responses that seem to have been specifically crafted to waste exponential CPU for a typical scraper regex…&lt;/sub&gt;&lt;/p&gt;</Body>
</answer>
</qroot>
